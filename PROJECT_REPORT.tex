\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}

% Correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Analysis of Economic Growth and Environmental Sustainability: A Data Engineering Approach}

\author{\IEEEauthorblockN{Dattathreya Chintalapudi}
\IEEEauthorblockA{\textit{MSc in Data Analytics} \\
\textit{National College of Ireland}\\
Dublin, Ireland \\
x24212881}
\and
\IEEEauthorblockN{Saatvik reddy Gutha}
\IEEEauthorblockA{\textit{MSc in Data Analytics} \\
\textit{National College of Ireland}\\
Dublin, Ireland \\
x24257460}
}

\maketitle

\begin{abstract}
In the contemporary discourse on climate change, a central tension exists between the imperative for economic development and the necessity of environmental preservation. This project interrogates this tension by developing a bespoke, end-to-end data engineering solution capable of ingesting, processing, and analyzing multi-source global datasets. Rather than relying on static, pre-packaged datasets, our team architected a live data pipeline that interfaces directly with the World Bank's Open Data API to harvest over 11,000 records spanning two decades (2000-2023).

The technical core of this solution is a ``Polyglot Persistence'' architecture, a design choice that leverages the strengths of both NoSQL (MongoDB) for flexible raw data ingestion and Relational (PostgreSQL) databases for structured analytical querying. We implemented a rigorously tested Extract-Transform-Load (ETL) pipeline using Python 3.10, which handles complex tasks such as recursive API pagination, schema flattening, and time-series interpolation for missing values. 

 Analytically, we deployed unsupervised Machine Learning (K-Means Clustering) to categorize nations into distinct sustainability profiles, revealing a ``Green Leader'' cluster that has successfully decoupled GDP growth from carbon emissions. Furthermore, a supervised Random Forest Regression model identified Energy Use as the dominant predictor of emissions (72\% feature importance), far outweighing population or economic size. These insights are delivered through a custom-built, interactive web dashboard using Plotly Dash, enabling stakeholders to explore the data dynamically. This report documents the complete software development lifecycle, from architectural design to statistical validation, highlighting the robust engineering practices employed to ensure data integrity and reproducibility.
\end{abstract}

\begin{IEEEkeywords}
Data Engineering, ETL Pipelines, Machine Learning, Polyglot Persistence, Environmental Kuznets Curve, Sustainability Analytics.
\end{IEEEkeywords}

\section{Introduction}

\subsection{Project Motivation}
The 21st century is defined by two opposing forces: the urgent need for economic development to lift billions out of poverty, and the existential threat of climate change driven by greenhouse gas emissions. Historically, these two forces have been inextricably linked; the Industrial Revolution demonstrated that economic output (GDP) is heavily correlated with energy consumption and, consequently, carbon dioxide (CO2) emissions. However, the rapid maturation of renewable energy technologies (solar, wind, hydroelectric) offers a potential pathway to ``green growth''—a theoretical framework where economies can grow without a corresponding increase in environmental damage.

For data scientists and policy analysts, the challenge is no longer a lack of data, but rather the fragmentation of it. While organizations like the World Bank, the IEA, and the OECD publish vast troves of data, it often resides in disparate silos, with varying formats, missing values, and inconsistent schemas. This project was motivated by the need to create a unified, automated framework that can ingest these disparate data sources and transform them into a ``Single Source of Truth'' for analysis.

\subsection{Research Question}
The analysis is guided by the following primary research question:
\begin{itemize}
    \item \textbf{RQ1:} To what extent does the adoption of renewable energy technologies decouple economic growth (GDP) from carbon emissions, and can unsupervised machine learning identify distinct typologies of national sustainability trajectories?
\end{itemize}

Secondary questions include:
\begin{itemize}
    \item \textbf{RQ2:} Which socio-economic factors (Population, Urbanization, GDP) are the strongest predictors of a nation's CO2 per capita?
    \item \textbf{RQ3:} Is there statistical evidence of the ``Environmental Kuznets Curve'' in the post-2000 era?
\end{itemize}

\subsection{Project Objectives}
To answer these questions, the project laid out the following technical and analytical objectives:
\begin{enumerate}
    \item \textbf{Automated Data Acquisition:} Develop Python scripts to programmatically fetch over 20 years of historical data for 30+ representative countries using the World Bank REST API.
    \item \textbf{Polyglot Database Architecture:} Implement a hybrid storage solution using MongoDB for raw data preservation (Data Lake) and PostgreSQL for analytical processing (Data Warehouse).
    \item \textbf{Robust ETL Pipeline:} Construct a reusable Extract-Transform-Load pipeline that handles data cleaning, pivoting, and merging, ensuring high data quality.
    \item \textbf{Advanced Analytics:} Apply statistical correlation methods and machine learning models (Clustering, Regression) to derive non-obvious insights.
    \item \textbf{Interactive Visualization:} Build a responsive dashboard that adheres to ``Shneiderman’s Mantra'' (Overview first, zoom and filter, details on demand) to make the data accessible.
\end{enumerate}

\section{Related Work}

\subsection{The Environmental Kuznets Curve (EKC)}
Theoretical groundwork for this analysis is found in the Environmental Kuznets Curve hypothesis. Stern \cite{stern2004rise} famously reviewed the critique that environmental degradation increases with income up to a turning point, after which it declines. Early studies relied on static, cross-sectional data, which often failed to capture dynamic technological shifts. Our project contributes to this body of work by using \textit{dynamic, longitudinal data} (2000-2023) to test if the ``turning point'' has shifted due to renewable technology.

\subsection{Modern Data Stack \& Polyglot Persistence}
In the realm of software architecture, this project is informed by the concept of ``Polyglot Persistence,'' popularized by Martin Fowler \cite{fowler2011polyglot}. The traditional approach of forcing all data into a single Relational Database Management System (RDBMS) is increasingly seen as an anti-pattern when dealing with hierarchical API data. By using MongoDB for the ``Ingestion Layer,'' we align with modern data engineering best practices that prioritize write throughput and schema flexibility at the start of the pipeline, shifting to strict schemas (PostgreSQL) only after the data is understood and cleaned.

\subsection{Interactive Visual Analytics}
Heer and Shneiderman \cite{heer2012interactive} established that ``interactive dynamics'' are crucial for data analysis. Static reports hide outliers and granular trends. By implementing a dashboard with filtering and ``brushing'' capabilities (where selecting a data point in one view highlights it in others), our project moves beyond static reporting to provide an exploratory tool, enabling users to discover their own insights.

\section{Data Processing Methodology}

\subsection{Data Acquisition Strategy}
The foundation of the project is the \texttt{src/data\_acquisition} module. We chose the \textbf{World Bank Open Data API} \cite{worldbank2024} as our primary source due to its reliability, comprehensive coverage, and open license.

\begin{itemize}
    \item \textbf{Dataset 1: Climate \& Energy:} This dataset tracks the environmental impact. Key indicators fetched include:
    \begin{itemize}
        \item \texttt{EN.ATM.CO2E.PC}: CO2 emissions (metric tons per capita).
        \item \texttt{EG.USE.PCAP.KG.OE}: Energy use (kg of oil equivalent per capita).
        \item \texttt{EG.FEC.RNEW.ZS}: Renewable energy consumption (\% of total final energy consumption).
    \end{itemize}
    \item \textbf{Dataset 2: Economic \& Demographic:} This dataset tracks development. Indicators include:
    \begin{itemize}
        \item \texttt{NY.GDP.PCAP.CD}: GDP per capita (current US\$).
        \item \texttt{SP.POP.TOTL}: Total Population.
        \item \texttt{SP.URB.TOTL.IN.ZS}: Urban population (\% of total).
    \end{itemize}
\end{itemize}

\textbf{Implementation Details:}
We created a base class \texttt{WorldBankFetcher} using the Python \texttt{requests} library. A critical challenge here was \textbf{Pagination}. The API returns data in pages of 50 records. Our script implements a recursive loop to detect the \texttt{total\_pages} metadata and iterate through all available pages to ensure complete data retrieval. We also implemented an \textbf{Exponential Backoff} strategy to handle API timeouts (retrying after 1s, 2s, 4s).

\subsection{Database Architecture (Storage)}
We implemented a \textbf{Tiered Storage Architecture} (see Fig. \ref{fig:architecture}):

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{trend_energy_use.png} 
\caption{Data Engineering Pipeline Architecture: From API to Dashboard}
\label{fig:architecture}
\end{figure}

\subsubsection{Tier 1: Raw Data Lake (MongoDB)}
We utilized MongoDB (hosted on MongoDB Atlas) \cite{mongodb2024} to store the raw API responses.
\begin{itemize}
    \item \textbf{Why MongoDB?} The World Bank API returns nested JSON objects. For example, the country field is not just a string but a dictionary: \texttt{\{"id": "US", "value": "United States"\}}. Flattening this immediately would risk losing context. Storing it as a ``Document'' allows us to preserve the exact state of the data as it was received.
    \item \textbf{Collection Strategy:} We separated data into three collections: \texttt{climate\_raw}, \texttt{economic\_raw}, and \texttt{renewable\_raw}. This logical separation improves query performance and organization.
\end{itemize}

\subsubsection{Tier 2: Analytical Data Warehouse (PostgreSQL)}
We utilized PostgreSQL (hosted on Neon Tech) \cite{postgresql2024} for the processed data.
\begin{itemize}
    \item \textbf{Why PostgreSQL?} The dashboard and ML models require structured, tabular data to perform joins and aggregations efficiently. SQL's strict typing (\texttt{FLOAT}, \texttt{INTEGER}, \texttt{VARCHAR}) acts as a final gatekeeper for data quality.
    \item \textbf{Schema Design:} We employed a Star Schema approach. A central fact table \texttt{combined\_analysis} holds the quantitative metrics, keyed by \texttt{country\_code} and \texttt{year}. This design is optimized for ``Online Analytical Processing'' (OLAP) workloads.
\end{itemize}

\subsection{The ETL Pipeline (Extract, Transform, Load)}
The core logic resides in \texttt{src/etl/pipeline.py}. This script orchestrates the flow of data:

\begin{enumerate}
    \item \textbf{Extract:} The \texttt{DataExtractor} class connects to MongoDB. It uses the aggregation pipeline to project only the necessary fields (\texttt{country.value}, \texttt{date}, \texttt{value}, \texttt{indicator.id}) from the deep JSON structure, returning a raw Pandas DataFrame.

    \item \textbf{Transform:} The \texttt{DataTransformer} class performs the heavy lifting:
    \begin{itemize}
        \item \textbf{Pivoting:} The raw data is in ``Long Format'' (Entity-Attribute-Value). We pivoted this to ``Wide Format'' using \texttt{pd.pivot\_table}, so that each indicator became a column (e.g., \texttt{climate\_co2}).
        \item \textbf{Data Cleaning:} We identified significant missing data in the 2022-2023 range for some indicators. We implemented a \textbf{Time-Series Interpolation} strategy. First, we used \texttt{ffill()} (Forward Fill) to propagate the last known valid observation forward to fill recent gaps. Then, we used \texttt{bfill()} (Backward Fill) for older gaps. This assumes that economic/climate metrics do not fluctuate wildly year-over-year, which is a statistically valid assumption for this domain.
        \item \textbf{Feature Engineering:} We created a derived feature \texttt{renewable\_adoption\_category}, binning countries into `Low', `Medium', and `High' based on their renewable percentage.
    \end{itemize}

    \item \textbf{Load:} The \texttt{DataLoader} class uses SQLAlchemy to insert the clean DataFrame into PostgreSQL. We used the \texttt{to\_sql(if\_exists='replace')} method to ensure the analysis table is completely refreshed on every pipeline run, preventing duplicate records.
\end{enumerate}

\subsection{Data Processing Challenges and Solutions}
Throughout the development lifecycle, we encountered several significant challenges that required specific engineering solutions:

\subsubsection{Challenge 1: Inconsistent API Data Availability}
\textit{Problem:} The World Bank API does not update all indicators simultaneously. While GDP data for 2023 might be available, CO2 emissions data often lags by 2-3 years. This resulted in \texttt{NaN} values when merging datasets on the \texttt{year} key, potentially causing our Machine Learning models to crash or drop entire years of valid economic data.

\textit{Solution:} We implemented a dynamic column checking mechanism in \texttt{src/analysis/ml\_models.py}. Before running clustering, the code explicitly checks \texttt{if 'climate\_co2\_per\_capita' in df.columns}. If the specific CO2 column is missing or entirely null for a given year, the model falls back to using \texttt{climate\_energy\_use} as a proxy variable. This ``graceful degradation'' ensures the pipeline completes even with partial data.

\subsubsection{Challenge 2: Cross-Module Import Resolution}
\textit{Problem:} As our project structure grew more complex with nested directories (\texttt{src/etl}, \texttt{src/analysis}, \texttt{config/}), Python scripts failed to import the \texttt{database\_config} module when run directly from the terminal, throwing \texttt{ModuleNotFoundError}.

\textit{Solution:} We implemented a programmatic path modification. In the header of every executable script, we added:
\begin{verbatim}
sys.path.insert(0, os.path.join(
    os.path.dirname(__file__), '../..'))
\end{verbatim}
This explicitly adds the project root to the Python path at runtime, ensuring reliable imports regardless of the working directory.

\subsubsection{Challenge 3: Database Connection Stability}
\textit{Problem:} During bulk insert operations to the cloud-hosted PostgreSQL database (Neon Tech), we occasionally experienced connection timeouts due to latency.

\textit{Solution:} We wrapped our \texttt{PostgresHandler} operations in a \texttt{try-finally} block. This ensures that the database cursor is always closed and the connection returned to the pool, preventing ``connection leak'' issues that would otherwise freeze the application after multiple runs.

\section{Data Visualisation Methodology}

\subsection{Theoretical Framework}
Our visualization strategy was grounded in \textbf{Tufte’s Principles of Graphical Integrity} \cite{tufte2001visual}. We aimed to maximize the ``Data-Ink Ratio'' by removing unnecessary grid lines and backgrounds, focusing the user's attention on the trends. We also applied \textbf{Gestalt Principles} of grouping, using color consistency (green for renewable, red for fossil fuels) across different charts to help the user cognitively link related data points.

\subsection{Dashboard Architecture}
The dashboard (\texttt{dashboard/app.py}) is a Single Page Application (SPA) built with \textbf{Dash Bootstrap Components} for a responsive grid layout \cite{plotly2024}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{trend_energy_use.png} 
\caption{Interactive Climate Analytics Dashboard Main View (Energy Trends)}
\label{fig:dashboard}
\end{figure}

\begin{itemize}
    \item \textbf{Global Controls:} A sidebar contains a ``Year Range'' slider and ``Country'' dropdown. These inputs trigger Python callbacks that query the PostgreSQL database in real-time.
    \item \textbf{Visual Components:}
    \begin{enumerate}
        \item \textbf{Main Trend Line (Time Series):} Visualizes the ``decoupling'' effect. By plotting Energy Use over time, users can see the trajectory of consumption.
        \item \textbf{Correlation Heatmap:} A 5x5 matrix showing Pearson correlations. We used a diverging 'RdBu' (Red-Blue) color scale. Red indicates positive correlation (GDP goes up, Energy goes up), while Blue indicates negative correlation. This immediately highlights the strong relationship between \texttt{climate\_energy\_use} and \texttt{climate\_fossil\_fuel\_consumption}.
        \item \textbf{Clustering Scatter Plot:} Displays the results of the K-Means analysis. Each point is a country-year, colored by its assigned cluster. This helps users visualize how countries move between clusters over time (e.g., moving from ``Developing'' to ``Green Leader'').
    \end{enumerate}
\end{itemize}

\section{Results and Evaluation}

\subsection{Statistical Findings}
The analysis of over 11,500 initial records (filtered down to $\approx$ 720 clean analysis rows) yielded robust statistical insights.
\begin{itemize}
    \item \textbf{The Energy-GDP Nexus:} We found a Pearson correlation of \textbf{0.82} between \texttt{GDP per Capita} and \texttt{Energy Use}. This confirms that despite efficiency gains, economic activity is still heavily energy-intensive.
    \item \textbf{The Renewable Gap:} The correlation between \texttt{GDP} and \texttt{Renewable Energy \%} is weak (\textbf{-0.15}). This indicates that wealth alone does not drive renewable adoption; policy decisions play a larger role. Some lower-GDP nations (like those in Latin America using hydropower) have higher renewable shares than wealthy fossil-fuel nations.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{correlation_heatmap.png} 
\caption{Pearson Correlation Matrix of Key Socio-Economic Indicators}
\label{fig:corr}
\end{figure}

\subsection{Machine Learning Analysis}
We executed a K-Means Clustering algorithm ($k=4$) to segment the nations.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{kmeans_clusters.png} 
\caption{K-Means Cluster Centroids: Grouping Countries by GDP and Renewable Adoption}
\label{fig:kmeans}
\end{figure}

\begin{itemize}
    \item \textbf{Cluster 0 (The Industrial Giants):} High GDP, High Energy Use, High CO2. (e.g., USA, Canada).
    \item \textbf{Cluster 1 (The Developing Majority):} Low GDP, Low Energy, Low CO2. (e.g., India, Vietnam).
    \item \textbf{Cluster 2 (The Green Leaders):} High GDP, High Energy, but Moderate CO2. This cluster represents the ``Target State'' for sustainable development.
    \item \textbf{Cluster 3 (Transition Economies):} Rapidly growing GDP with exploding emissions.
\end{itemize}

\textbf{Regression Analysis:}
A Random Forest Regressor ($n\_estimators=100$) \cite{scikit-learn} was trained to predict CO2 emissions.
\begin{itemize}
    \item \textbf{$R^2$ Score:} 0.94 (The model explains 94\% of the variance in emissions).
    \item \textbf{Feature Importance:}
    \begin{enumerate}
        \item Energy Use per Capita (72\%)
        \item Fossil Fuel Consumption (18\%)
        \item GDP per Capita (5\%)
        \item Urbanization (3\%)
    \end{enumerate}
    \item \textit{Interpretation:} Economic growth (GDP) is only a distal cause of emissions. The proximal cause is energy intensity. Policy should focus on energy efficiency (reducing the need for energy) rather than just degrowth.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{feature_importance.png} 
\caption{Random Forest Feature Importance for Predicting CO2 Emissions}
\label{fig:importance}
\end{figure}

\subsection{Evaluation of Objectives}
\begin{itemize}
    \item \textbf{Objective 1 (Pipeline):} Met. The system successfully ingested over 11,500 records from the API.
    \item \textbf{Objective 2 (Database):} Met. Data exists in both MongoDB (Raw) and Postgres (Processed), verifying the Polyglot architecture.
    \item \textbf{Objective 3 (ML):} Met. Clusters successfully differentiate country types, and Regression $R^2$ score $>$ 0.90 indicates high predictive accuracy.
    \item \textbf{Objective 4 (Dashboard):} Met. The application renders interactive charts with sub-second latency.
\end{itemize}

\section{Conclusions and Future Work}

\subsection{Conclusion}
This project successfully demonstrated the application of advanced data engineering to the domain of climate economics. By automating the data lifecycle, we removed the friction typically associated with cross-national studies. 
Our findings challenge the simplistic view that growth equals pollution. The identification of ``Cluster 2'' (Green Leaders) proves that high living standards are compatible with lower emissions, provided there is a structural shift in energy sources. The ``Polyglot'' architecture proved highly effective, offering the best of both worlds: the agility of NoSQL for ingestion and the rigor of SQL for analysis.

\subsection{Limitations}
\begin{itemize}
    \item \textbf{Data Lag:} The World Bank data often lags by 1-2 years. Our interpolation methods, while statistically valid, are approximations of the current reality.
    \item \textbf{Scope:} We focused on 30 representative countries. A global analysis of 190+ countries would require more sophisticated distributed processing (e.g., Apache Spark) to handle the increased load.
\end{itemize}

\subsection{Future Work}
\begin{enumerate}
    \item \textbf{Predictive Forecasting:} We plan to implement LSTM (Long Short-Term Memory) Recurrent Neural Networks to forecast CO2 trends through 2030, allowing for ``What-If'' scenario planning.
    \item \textbf{Live Data Integration:} Integrating real-time electricity grid APIs (like ElectricityMaps) would allow for daily, rather than annual, monitoring of carbon intensity.
    \item \textbf{Cloud Deployment:} Containerizing the application with Docker and deploying it to a cloud provider (AWS EC2 or Heroku) would make the dashboard accessible to the public.
\end{enumerate}

\begin{thebibliography}{00}
\bibitem{stern2004rise} D. I. Stern, ``The Rise and Fall of the Environmental Kuznets Curve,'' \textit{World Development}, vol. 32, no. 8, pp. 1419-1439, 2004.
\bibitem{fowler2011polyglot} M. Fowler, ``Polyglot Persistence,'' 2011. [Online]. Available: \url{https://martinfowler.com/bliki/PolyglotPersistence.html}.
\bibitem{heer2012interactive} J. Heer and B. Shneiderman, ``Interactive Dynamics for Visual Analysis,'' \textit{Queue}, vol. 10, no. 2, pp. 30, 2012.
\bibitem{worldbank2024} World Bank Group, ``World Bank Open Data API,'' 2024. [Online]. Available: \url{https://data.worldbank.org/}.
\bibitem{scikit-learn} F. Pedregosa et al., ``Scikit-learn: Machine Learning in Python,'' \textit{Journal of Machine Learning Research}, vol. 12, pp. 2825-2830, 2011.
\bibitem{postgresql2024} PostgreSQL Global Development Group, ``PostgreSQL 16 Documentation,'' 2024.
\bibitem{mongodb2024} MongoDB Inc., ``The MongoDB 6.0 Manual,'' 2024.
\bibitem{plotly2024} Plotly Technologies Inc., ``Dash User Guide,'' 2024. [Online]. Available: \url{https://dash.plotly.com/}.
\bibitem{mckinney2010} W. McKinney, ``Data Structures for Statistical Computing in Python,'' in \textit{Proceedings of the 9th Python in Science Conference}, 2010.
\bibitem{tufte2001visual} E. R. Tufte, \textit{The Visual Display of Quantitative Information}. Cheshire, CT: Graphics Press, 2001.
\end{thebibliography}

\appendices
\section{Detailed Test Results}

\subsection{Functional Testing}
\begin{table}[htbp]
\caption{Functional Testing Summary}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{ID} & \textbf{Feature} & \textbf{Expected} & \textbf{Actual} & \textbf{Status} \\
\hline
TC-001 & API Connect & 200 OK & 200 OK & Pass \\
\hline
TC-002 & Mongo Write & Docs $>$ 0 & 11,520 Docs & Pass \\
\hline
TC-003 & Cleaning & No NaNs & 0 NaNs & Pass \\
\hline
TC-004 & Postgres & Tables Exist & Created & Pass \\
\hline
TC-005 & ML Train & $R^2$ Score & 0.94 & Pass \\
\hline
TC-006 & Dashboard & HTTP 200 & HTTP 200 & Pass \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\subsection{Security \& Error Handling}
\begin{table}[htbp]
\caption{Security \& Error Handling Tests}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{ID} & \textbf{Feature} & \textbf{Expected} & \textbf{Actual} & \textbf{Status} \\
\hline
SEC-01 & Env Keys & No Keys & .env used & Pass \\
\hline
ERR-01 & DB Timeout & Log Error & Logged & Pass \\
\hline
ERR-02 & Bad Data & Skip Rec & Skipped & Pass \\
\hline
\end{tabular}
\label{tab2}
\end{center}
\end{table}

\end{document}
